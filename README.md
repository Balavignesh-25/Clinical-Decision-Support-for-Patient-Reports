# Secure Medical Report Analysis 

This project implements a **secure, end-to-end medical document analysis pipeline** using **Groq‚Äôs LLaMA 3.1** model in **Google Colab**, following **industry best practices for PHI handling and API security**.

It processes patient PDF reports, extracts clinical information using OCR and LLMs, anonymizes sensitive data, and generates structured medical insights for clinical decision support.

---

## üöÄ Key Features

- üìÑ PDF medical report ingestion
- üîç OCR-based text extraction (Tesseract + Poppler)
- üîê PHI anonymization with reversible token mapping
- üß† Structured clinical event extraction using LLaMA 3.1
- üìä Chronological medical history synthesis
- üö® Abnormality detection with severity assessment
- üîÑ Optional PHI re-identification (controlled)
- üîë Secure API key handling (no hardcoding)
- ‚òÅÔ∏è Google Colab & GitHub safe

---

## üèóÔ∏è System Architecture (High Level)
```
PDF Reports
‚Üì
OCR (Tesseract)
‚Üì
PHI Anonymization
‚Üì
LLaMA 3.1 (Groq API)
‚Üì
Structured JSON Outputs
‚Üì
Clinical Summary & Analysis
```

## üîê API Key & Security (IMPORTANT)

### Why this matters
Medical data + API keys = **high-risk**  
This project is designed to be **safe for GitHub** and compliant with secure development practices.

### ‚úÖ Google Colab (Recommended)

1. Open the notebook in **Google Colab**
2. Click **üîë Secrets** (left sidebar)
3. Add a new secret:
   - **Name**: `GROQ_API_KEY`
   - **Value**: your Groq API key
4. Enable **Notebook access**
5. Restart the runtime
6. Run all cells

> ‚ö†Ô∏è The API key is **never stored in code or notebooks** and is **not committed to GitHub**.

---

## üñ•Ô∏è Local Development (Optional)

If you want to run locally:

### Set environment variable

**Linux / macOS**
```bash
export GROQ_API_KEY=your_api_key_here
```
Windows (PowerShell)
```
setx GROQ_API_KEY "your_api_key_here"
```

---

## üß™ What the Pipeline Extracts

### From historical reports

- Past diseases
- Previous medications
- Prior consultations

### From the latest report

- Abnormal lab values
- Severity classification (mild / moderate / severe)
- General (non-prescriptive) recommendations
- Doctor consultation urgency

---

## üîí PHI Handling Strategy

- Sensitive fields (names, phone numbers, emails) are tokenized
- Tokens are passed to the LLM instead of raw PHI
- Optional controlled re-identification is supported
- Designed with healthcare privacy principles in mind

##‚ö†Ô∏è Disclaimer

- This project is intended for educational and research purposes only.
- It is not a medical diagnosis system
- It does not replace licensed medical professionals
- Any recommendations generated are informational only

---

## üß† Engineering Highlights

- Deterministic LLM outputs (temperature=0)
- Safe JSON parsing to prevent hallucinated responses
- Modular, testable functions
- Environment-aware secret management
- GitHub & recruiter friendly

---

## üöÄ Future Enhancements

- RAG with clinical guidelines (WHO / NICE / CDC)
- FastAPI backend for hospital systems
- Streamlit dashboard for doctors
- Confidence scoring for extracted insights
- Audit logs for compliance

---

## ‚≠ê Final Note

This repository demonstrates real-world ML engineering practices:
- Security-first mindset
- Healthcare-aware design
- Production-oriented code structure

---
